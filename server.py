#!/usr/bin/env python3
"""
Servidor F5-TTS Otimizado para GPU
Vers√£o com suporte CUDA e otimiza√ß√µes de mem√≥ria
"""

import os
import sys
import logging
import tempfile
import shutil
from pathlib import Path
from typing import Optional
import time
import gc

import torch
import torchaudio
import librosa
import numpy as np
from flask import Flask, request, jsonify, send_file
from werkzeug.utils import secure_filename
import soundfile as sf
from huggingface_hub import hf_hub_download, snapshot_download

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GPUOptimizedF5TTSServer:
    def __init__(self):
        self.model_name = "Tharyck/multispeaker-ptbr-f5tts"
        self.setup_device()
        self.sample_rate = 22050
        self.model = None
        self.model_dir = "/app/models"
        
        logger.info(f"üöÄ Inicializando servidor F5-TTS GPU-otimizado")
        logger.info(f"üì± Dispositivo: {self.device}")
        
        # Configurar otimiza√ß√µes CUDA
        self.setup_cuda_optimizations()
        
        # Criar diret√≥rios necess√°rios
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs("/app/temp", exist_ok=True)
        
        # Baixar modelo na inicializa√ß√£o
        self.download_model()
    
    def setup_device(self):
        """Configurar dispositivo com verifica√ß√µes detalhadas"""
        if torch.cuda.is_available():
            self.device = "cuda"
            self.gpu_count = torch.cuda.device_count()
            self.gpu_name = torch.cuda.get_device_name(0)
            self.gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            logger.info(f"üéÆ GPU detectada: {self.gpu_name}")
            logger.info(f"üìä Mem√≥ria GPU: {self.gpu_memory:.1f}GB")
            logger.info(f"üî¢ GPUs dispon√≠veis: {self.gpu_count}")
            
            # Verificar se h√° mem√≥ria suficiente
            if self.gpu_memory < 4.0:
                logger.warning(f"‚ö†Ô∏è  GPU com pouca mem√≥ria ({self.gpu_memory:.1f}GB). Recomendado: 6GB+")
            
        else:
            self.device = "cpu"
            logger.warning("üñ•Ô∏è  GPU n√£o dispon√≠vel - usando CPU")
            logger.info("üí° Para usar GPU:")
            logger.info("   1. Instale drivers NVIDIA")
            logger.info("   2. Configure nvidia-container-toolkit") 
            logger.info("   3. Use docker-compose-gpu.yml")
    
    def setup_cuda_optimizations(self):
        """Configurar otimiza√ß√µes CUDA"""
        if self.device == "cuda":
            # Otimiza√ß√µes de mem√≥ria
            torch.cuda.empty_cache()
            
            # Configurar mixed precision se dispon√≠vel
            if hasattr(torch.cuda, 'amp'):
                self.use_mixed_precision = True
                logger.info("‚ö° Mixed precision habilitada")
            else:
                self.use_mixed_precision = False
            
            # Configurar cuDNN para performance
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # Configurar alocador de mem√≥ria
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
            
            logger.info("üîß Otimiza√ß√µes CUDA configuradas")
    
    def get_gpu_memory_info(self):
        """Obter informa√ß√µes de mem√≥ria GPU"""
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            reserved = torch.cuda.memory_reserved(0) / (1024**3)
            total = self.gpu_memory
            
            return {
                'allocated_gb': round(allocated, 2),
                'reserved_gb': round(reserved, 2),
                'total_gb': round(total, 2),
                'free_gb': round(total - reserved, 2),
                'usage_percent': round((reserved / total) * 100, 1)
            }
        return None
    
    def clear_gpu_memory(self):
        """Limpar mem√≥ria GPU"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("üßπ Mem√≥ria GPU limpa")
    
    def download_model(self):
        """Download do modelo do HuggingFace"""
        try:
            logger.info(f"üì• Baixando modelo: {self.model_name}")
            
            # Tentar download do snapshot completo
            try:
                model_path = snapshot_download(
                    repo_id=self.model_name,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False
                )
                logger.info(f"‚úÖ Modelo baixado em: {model_path}")
                
            except Exception as e:
                logger.warning(f"Erro no snapshot download: {e}")
                logger.info("Tentando download individual de arquivos...")
                
                # Lista de arquivos essenciais
                files_to_download = [
                    "config.json",
                    "model.safetensors", 
                    "pytorch_model.bin",
                    "tokenizer.json",
                    "vocab.json"
                ]
                
                for filename in files_to_download:
                    try:
                        file_path = hf_hub_download(
                            repo_id=self.model_name,
                            filename=filename,
                            local_dir=self.model_dir,
                            local_dir_use_symlinks=False
                        )
                        logger.info(f"üì¶ Baixado: {filename}")
                    except Exception as file_error:
                        logger.warning(f"N√£o foi poss√≠vel baixar {filename}: {file_error}")
            
            self.load_model()
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar modelo: {e}")
            logger.warning("üîÑ Usando modo simula√ß√£o")
    
    def load_model(self):
        """Carrega o modelo F5-TTS com otimiza√ß√µes GPU"""
        try:
            logger.info("üîÑ Carregando modelo...")
            
            # Limpar mem√≥ria antes de carregar
            self.clear_gpu_memory()
            
            # IMPLEMENTA√á√ÉO REAL: Descomente quando F5-TTS estiver dispon√≠vel
            # from f5_tts import F5TTS
            # 
            # # Configurar dtype baseado na GPU
            # if self.device == "cuda":
            #     if self.use_mixed_precision:
            #         torch_dtype = torch.float16
            #         logger.info("üéØ Usando float16 para economia de mem√≥ria")
            #     else:
            #         torch_dtype = torch.float32
            # else:
            #     torch_dtype = torch.float32
            # 
            # # Carregar modelo
            # self.model = F5TTS.from_pretrained(
            #     self.model_dir,
            #     torch_dtype=torch_dtype,
            #     device_map=self.device
            # )
            # 
            # # Otimizar modelo para infer√™ncia
            # if self.device == "cuda":
            #     self.model = torch.compile(self.model, mode="reduce-overhead")
            #     logger.info("‚ö° Modelo compilado com torch.compile")
            
            # Para demonstra√ß√£o, usar placeholder
            self.model = "placeholder_model_gpu"
            
            # Log de mem√≥ria ap√≥s carregamento
            if self.device == "cuda":
                memory_info = self.get_gpu_memory_info()
                logger.info(f"üìä Mem√≥ria GPU ap√≥s carregamento: {memory_info['usage_percent']}% usada")
            
            logger.info("‚úÖ Modelo carregado com sucesso!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            self.model = None
    
    def preprocess_audio_gpu(self, audio_file: str) -> torch.Tensor:
        """Pr√©-processa √°udio de refer√™ncia com GPU"""
        try:
            # Carregar √°udio
            audio, sr = librosa.load(audio_file, sr=self.sample_rate)
            
            # Converter para tensor e enviar para GPU
            audio_tensor = torch.FloatTensor(audio)
            
            if self.device == "cuda":
                audio_tensor = audio_tensor.cuda()
            
            # Normalizar
            audio_tensor = audio_tensor / torch.max(torch.abs(audio_tensor))
            
            # Remover sil√™ncio (usando torch operations para GPU)
            # Implementa√ß√£o simplificada - usar librosa.effects.trim em produ√ß√£o
            
            # Garantir dura√ß√£o m√≠nima/m√°xima
            min_samples = int(1.0 * self.sample_rate)  # 1 segundo m√≠nimo
            max_samples = int(30.0 * self.sample_rate)  # 30 segundos m√°ximo
            
            if audio_tensor.shape[0] < min_samples:
                # Repetir √°udio se muito curto
                repeats = int(np.ceil(min_samples / audio_tensor.shape[0]))
                audio_tensor = audio_tensor.repeat(repeats)[:min_samples]
            
            if audio_tensor.shape[0] > max_samples:
                # Truncar se muito longo
                audio_tensor = audio_tensor[:max_samples]
            
            logger.info(f"üéµ √Åudio processado: {audio_tensor.shape[0]/self.sample_rate:.2f}s")
            return audio_tensor
            
        except Exception as e:
            logger.error(f"‚ùå Erro no pr√©-processamento GPU: {e}")
            raise
    
    @torch.inference_mode()  # Otimiza√ß√£o para infer√™ncia
    def synthesize_voice_gpu(self, text: str, reference_audio: Optional[torch.Tensor] = None) -> np.ndarray:
        """Sintetiza voz com otimiza√ß√µes GPU"""
        try:
            logger.info(f"üé§ Sintetizando com GPU: '{text[:50]}...'")
            
            # Limpar mem√≥ria antes da s√≠ntese
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            if self.model and self.model != "placeholder_model_gpu":
                # IMPLEMENTA√á√ÉO REAL: Descomente quando F5-TTS estiver dispon√≠vel
                # with torch.cuda.amp.autocast(enabled=self.use_mixed_precision):
                #     if reference_audio is not None:
                #         # Clonagem de voz
                #         audio = self.model.synthesize_with_reference(
                #             text=text,
                #             reference_audio=reference_audio,
                #             sample_rate=self.sample_rate
                #         )
                #     else:
                #         # S√≠ntese normal
                #         audio = self.model.synthesize(
                #             text=text,
                #             sample_rate=self.sample_rate
                #         )
                # 
                # # Converter para CPU para salvar
                # if isinstance(audio, torch.Tensor):
                #     audio = audio.cpu().numpy()
                # 
                # return audio
                pass
            
            # SIMULA√á√ÉO GPU-otimizada para demonstra√ß√£o
            logger.info("üîÑ Gerando √°udio simulado com acelera√ß√£o GPU...")
            
            # Usar GPU para c√°lculos se dispon√≠vel
            duration = max(2.0, len(text) * 0.08)
            n_samples = int(self.sample_rate * duration)
            
            if self.device == "cuda":
                # Gerar no GPU
                t = torch.linspace(0, duration, n_samples, device='cuda')
                
                # Frequ√™ncia base
                base_freq = 150.0
                
                # Se h√° refer√™ncia, analisar no GPU
                if reference_audio is not None:
                    # An√°lise FFT no GPU
                    ref_fft = torch.fft.rfft(reference_audio)
                    dominant_freq_idx = torch.argmax(torch.abs(ref_fft))
                    estimated_pitch = float(dominant_freq_idx * self.sample_rate / (2 * len(ref_fft)))
                    base_freq = max(80.0, min(400.0, estimated_pitch))
                    logger.info(f"üéØ Pitch estimado (GPU): {base_freq:.1f} Hz")
                
                # Gerar sinal no GPU
                audio = torch.zeros_like(t)
                
                # Componente fundamental
                audio += 0.3 * torch.sin(2 * np.pi * base_freq * t)
                
                # Harm√¥nicos
                for harm in [2, 3, 4, 5]:
                    amplitude = 0.1 / harm
                    audio += amplitude * torch.sin(2 * np.pi * base_freq * harm * t)
                
                # Modula√ß√£o
                vibrato_freq = 4.5
                vibrato_depth = 0.02
                vibrato = 1 + vibrato_depth * torch.sin(2 * np.pi * vibrato_freq * t)
                audio *= vibrato
                
                # Envelope
                fade_samples = int(0.1 * self.sample_rate)
                if len(audio) > 2 * fade_samples:
                    fade_in = torch.linspace(0, 1, fade_samples, device='cuda')
                    fade_out = torch.linspace(1, 0, fade_samples, device='cuda')
                    audio[:fade_samples] *= fade_in
                    audio[-fade_samples:] *= fade_out
                
                # Ru√≠do no GPU
                noise = torch.randn_like(audio) * 0.01
                audio += noise
                
                # Normalizar
                audio = audio / torch.max(torch.abs(audio)) * 0.8
                
                # Converter para CPU/numpy
                audio = audio.cpu().numpy().astype(np.float32)
                
            else:
                # Fallback para CPU (c√≥digo original)
                t = np.linspace(0, duration, n_samples)
                base_freq = 150
                
                if reference_audio is not None:
                    ref_audio_np = reference_audio.cpu().numpy() if isinstance(reference_audio, torch.Tensor) else reference_audio
                    ref_fft = np.fft.rfft(ref_audio_np)
                    dominant_freq_idx = np.argmax(np.abs(ref_fft))
                    estimated_pitch = dominant_freq_idx * self.sample_rate / (2 * len(ref_fft))
                    base_freq = max(80, min(400, estimated_pitch))
                
                audio = np.zeros_like(t)
                audio += 0.3 * np.sin(2 * np.pi * base_freq * t)
                
                for harm in [2, 3, 4, 5]:
                    amplitude = 0.1 / harm
                    audio += amplitude * np.sin(2 * np.pi * base_freq * harm * t)
                
                vibrato = 1 + 0.02 * np.sin(2 * np.pi * 4.5 * t)
                audio *= vibrato
                
                fade_samples = int(0.1 * self.sample_rate)
                if len(audio) > 2 * fade_samples:
                    audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
                    audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)
                
                noise = np.random.normal(0, 0.01, len(audio))
                audio += noise
                audio = (audio / np.max(np.abs(audio)) * 0.8).astype(np.float32)
            
            # Limpar mem√≥ria ap√≥s s√≠ntese
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            logger.info("‚úÖ S√≠ntese GPU conclu√≠da!")
            return audio
            
        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese GPU: {e}")
            raise

# Criar app Flask
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MB max

# Inicializar servidor GPU-otimizado
tts_server = GPUOptimizedF5TTSServer()

@app.route('/health', methods=['GET'])
def health_check():
    """Verifica√ß√£o de sa√∫de com info GPU"""
    gpu_info = tts_server.get_gpu_memory_info()
    
    response = {
        'status': 'healthy',
        'device': tts_server.device,
        'model_loaded': tts_server.model is not None,
        'version': '1.0.0-gpu'
    }
    
    if gpu_info:
        response['gpu'] = {
            'name': tts_server.gpu_name,
            'memory': gpu_info,
            'count': tts_server.gpu_count
        }
    
    return jsonify(response)

@app.route('/gpu-status', methods=['GET'])
def gpu_status():
    """Status detalhado da GPU"""
    if tts_server.device != "cuda":
        return jsonify({'error': 'GPU n√£o dispon√≠vel'}), 400
    
    return jsonify({
        'gpu_name': tts_server.gpu_name,
        'memory': tts_server.get_gpu_memory_info(),
        'cuda_version': torch.version.cuda,
        'pytorch_version': torch.__version__,
        'mixed_precision': tts_server.use_mixed_precision
    })

@app.route('/clear-cache', methods=['POST'])
def clear_cache():
    """Limpar cache GPU"""
    tts_server.clear_gpu_memory()
    return jsonify({'status': 'Cache GPU limpo'})

@app.route('/synthesize', methods=['POST'])
def synthesize():
    """Endpoint para s√≠ntese de voz GPU-otimizada"""
    try:
        # Log mem√≥ria antes do processamento
        if tts_server.device == "cuda":
            mem_before = tts_server.get_gpu_memory_info()
            logger.info(f"üíæ Mem√≥ria GPU antes: {mem_before['usage_percent']}%")
        
        # Verificar texto
        text = request.form.get('text', '').strip()
        if not text:
            return jsonify({'error': 'Texto √© obrigat√≥rio'}), 400
        
        logger.info(f"üìù Novo request GPU: {text[:50]}...")
        
        reference_audio = None
        
        # Processar √°udio de refer√™ncia
        if 'audio' in request.files:
            audio_file = request.files['audio']
            if audio_file.filename:
                filename = secure_filename(audio_file.filename)
                temp_path = f"/app/temp/{int(time.time())}_{filename}"
                audio_file.save(temp_path)
                
                logger.info(f"üéµ Processando √°udio com GPU: {filename}")
                
                try:
                    reference_audio = tts_server.preprocess_audio_gpu(temp_path)
                except Exception as e:
                    logger.error(f"Erro no √°udio de refer√™ncia: {e}")
                    return jsonify({'error': f'Erro no √°udio: {str(e)}'}), 400
                finally:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
        
        # Sintetizar com GPU
        try:
            synthesized_audio = tts_server.synthesize_voice_gpu(text, reference_audio)
        except Exception as e:
            logger.error(f"Erro na s√≠ntese GPU: {e}")
            return jsonify({'error': f'Erro na s√≠ntese: {str(e)}'}), 500
        
        # Salvar resultado
        output_path = f"/app/temp/output_{int(time.time())}.wav"
        sf.write(output_path, synthesized_audio, tts_server.sample_rate)
        
        # Log mem√≥ria ap√≥s processamento
        if tts_server.device == "cuda":
            mem_after = tts_server.get_gpu_memory_info()
            logger.info(f"üíæ Mem√≥ria GPU depois: {mem_after['usage_percent']}%")
        
        logger.info("‚úÖ S√≠ntese GPU conclu√≠da com sucesso!")
        
        return send_file(
            output_path,
            as_attachment=True,
            download_name='synthesized_voice_gpu.wav',
            mimetype='audio/wav'
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erro interno GPU: {e}")
        return jsonify({'error': f'Erro interno: {str(e)}'}), 500

@app.route('/clone', methods=['POST'])
def clone_voice():
    """Endpoint espec√≠fico para clonagem de voz"""
    return synthesize()  # Redirecionar para endpoint principal

@app.route('/', methods=['GET'])
def index():
    """P√°gina inicial com informa√ß√µes GPU"""
    gpu_info = ""
    if tts_server.device == "cuda":
        memory = tts_server.get_gpu_memory_info()
        gpu_info = f"""
        <div class="container">
            <h3>üéÆ Status da GPU</h3>
            <p><strong>GPU:</strong> {tts_server.gpu_name}</p>
            <p><strong>Mem√≥ria:</strong> {memory['usage_percent']}% usada ({memory['allocated_gb']:.1f}GB / {memory['total_gb']:.1f}GB)</p>
            <p><strong>CUDA:</strong> {torch.version.cuda}</p>
            <p><strong>Mixed Precision:</strong> {'‚úÖ' if tts_server.use_mixed_precision else '‚ùå'}</p>
            <button onclick="clearGPUCache()">üßπ Limpar Cache GPU</button>
        </div>
        """
    
    return f'''
    <!DOCTYPE html>
    <html>
    <head>
        <title>F5-TTS GPU Server</title>
        <style>
            body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
            .container {{ background: #f5f5f5; padding: 20px; border-radius: 10px; margin: 20px 0; }}
            .gpu-container {{ background: #e3f2fd; border-left: 4px solid #2196f3; }}
            textarea {{ width: 100%; height: 100px; margin: 10px 0; }}
            input[type="file"] {{ margin: 10px 0; }}
            button {{ background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; margin: 5px; }}
            button:hover {{ background: #0056b3; }}
            .status {{ padding: 10px; margin: 10px 0; border-radius: 5px; }}
            .success {{ background: #d4edda; border: 1px solid #c3e6cb; color: #155724; }}
            .error {{ background: #f8d7da; border: 1px solid #f5c6cb; color: #721c24; }}
            audio {{ width: 100%; margin: 10px 0; }}
            .gpu-badge {{ background: #28a745; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; }}
        </style>
    </head>
    <body>
        <h1>üéôÔ∏è F5-TTS Voice Cloning Server <span class="gpu-badge">GPU</span></h1>
        
        {gpu_info}
        
        <div class="container">
            <h3>üìä Status do Servidor</h3>
            <p id="status">Carregando...</p>
        </div>
        
        <div class="container">
            <h3>üé§ S√≠ntese de Voz GPU-Acelerada</h3>
            <form id="ttsForm" enctype="multipart/form-data">
                <label>üìù Texto:</label><br>
                <textarea id="text" placeholder="Digite o texto para sintetizar..." required></textarea><br>
                
                <label>üéµ √Åudio de Refer√™ncia (opcional para clonagem):</label><br>
                <input type="file" id="audio" accept="audio/*"><br>
                
                <button type="submit">üöÄ Sintetizar com GPU</button>
            </form>
            
            <div id="result"></div>
        </div>
        
        <script>
        // Verificar status do servidor
        fetch('/health')
            .then(r => r.json())
            .then(data => {{
                let gpu_info = '';
                if (data.gpu) {{
                    gpu_info = `<br><strong>GPU:</strong> ${{data.gpu.name}}<br><strong>Mem√≥ria GPU:</strong> ${{data.gpu.memory.usage_percent}}%`;
                }}
                document.getElementById('status').innerHTML = `
                    <strong>Status:</strong> ${{data.status}}<br>
                    <strong>Dispositivo:</strong> ${{data.device}}<br>
                    <strong>Modelo:</strong> ${{data.model_loaded ? '‚úÖ Carregado' : '‚ùå N√£o carregado'}}<br>
                    <strong>Vers√£o:</strong> ${{data.version}}${{gpu_info}}
                `;
            }});
        
        // Limpar cache GPU
        function clearGPUCache() {{
            fetch('/clear-cache', {{method: 'POST'}})
                .then(r => r.json())
                .then(data => alert('‚úÖ Cache GPU limpo!'))
                .catch(e => alert('‚ùå Erro ao limpar cache'));
        }}
        
        // Formul√°rio de s√≠ntese
        document.getElementById('ttsForm').onsubmit = async (e) => {{
            e.preventDefault();
            
            const formData = new FormData();
            formData.append('text', document.getElementById('text').value);
            
            const audioFile = document.getElementById('audio').files[0];
            if (audioFile) {{
                formData.append('audio', audioFile);
            }}
            
            const resultDiv = document.getElementById('result');
            resultDiv.innerHTML = '<div class="status">üîÑ Processando com GPU...</div>';
            
            try {{
                const response = await fetch('/synthesize', {{
                    method: 'POST',
                    body: formData
                }});
                
                if (response.ok) {{
                    const blob = await response.blob();
                    const url = URL.createObjectURL(blob);
                    
                    resultDiv.innerHTML = `
                        <div class="status success">‚úÖ S√≠ntese GPU conclu√≠da!</div>
                        <audio controls>
                            <source src="${{url}}" type="audio/wav">
                        </audio>
                        <br>
                        <a href="${{url}}" download="voice_cloned_gpu.wav">
                            <button>üì• Download</button>
                        </a>
                    `;
                }} else {{
                    const error = await response.json();
                    resultDiv.innerHTML = `<div class="status error">‚ùå Erro: ${{error.error}}</div>`;
                }}
            }} catch (error) {{
                resultDiv.innerHTML = `<div class="status error">‚ùå Erro de conex√£o: ${{error}}</div>`;
            }}
        }};
        </script>
    </body>
    </html>
    '''

if __name__ == '__main__':
    logger.info("üåê Iniciando servidor GPU...")
    app.run(host='0.0.0.0', port=8000, debug=False)